# Generative Models Tutorial using PyTorch

[colab-badge]: <https://colab.research.google.com/assets/colab-badge.svg>
[![Open In Colab][colab-badge]](https://colab.research.google.com/github/hasibzunair/ericsson-upskill-tutorials/blob/main/tensorflow_tutorial.ipynb)


Tutorial materials for **Ericsson AI & ML Upskill Program 2023-2024**. It contains code for finetuning a GPT-like large language model (LLM) in pure PyTorch. 

Materials for the previous program can be found [Ericsson AI & ML Upskill Program 2021](https://github.com/hasibzunair/intro-ml-tutorial).

### Learning resources
* [Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g?si=bIVqXiQratJdqvhj)
* [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)
* [QLoRA is all you need (Fast and lightweight model fine-tuning)](https://youtu.be/J_3hDqSvpmg?si=b0j8O2puV42z18UL)
* [Parameter-efficient Finetuning with LoRA](https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb)
* [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
* [Run Mixtral-8x7B models in Colab or consumer desktops](https://github.com/dvmazur/mixtral-offloading)

### Acknowledgements

This repository was built using [Implementing a ChatGPT-like LLM in PyTorch from scratch, step by step](https://github.com/rasbt/LLMs-from-scratch).
